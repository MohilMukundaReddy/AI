import numpy as np
import cv2





class Environment:
    def __init__(self, P, gamma=1.0):
        self.P = P
        self.gamma = gamma
        self.terminal_states = self.find_terminal_states()

    def find_terminal_states(self):
        terminal_states = set()

        for state, outcomes in self.P.items():
            is_terminal = all(isinstance(outcome, tuple) and outcome[2] == state for action, outcomes in outcomes.items() for outcome in outcomes[action])
            if is_terminal:
                terminal_states.add(state)

        return list(terminal_states)

    def get_actions(self, state):
        return list(self.P[state].keys())

    def get_next_state_info(self, state, action):
        return self.P[state][action]

    def random_state(self):
        return np.random.choice(list(self.P.keys()))

# Define the transition probabilities (P) for the example
P = {
    'A': {0: [(1.0, 0, 'A', False)], 1: [(1.0, 1, 'B', False)]},
    'B': {0: [(1.0, -2, 'A', False)], 1: [(1.0, 1, 'C', False)]},
    'C': {0: [(1.0, -2, 'B', False)], 1: [(0.5, 1, 'D', False), (0.5, 4, 'E', False)]},
    'D': {0: [(1.0, -2, 'C', False)], 1: [(1.0, 1, 'E', False)]},
    'E': {0: [(1.0, -2, 'D', False)], 1: [(1.0, 1, 'F', False)]},
    'F': {0: [(1.0, -2, 'E', False)], 1: [(1.0, 1, 'G', True)]},
    'G': {0: [(1.0, 0, 'G', True), (1.0, 0, 'G', True)]}
}


class FirstVisitMonteCarlo:
    def __init__(self, env):
        self.env = env

    def simulate_with_trajectories(self, trajectories):
        values = {state: 0 for state in self.env.P.keys()}
        visits = {state: 0 for state in self.env.P.keys()}

        for trajectory in trajectories:
            visited_states = set()
            for i in range(len(trajectory)//2 ): 
                state = trajectory[i*2]
                reward = trajectory[i*2 + 1]
                if state not in visited_states:
                    visited_states.add(state)
                for visited_state in visited_states:
                    values[visited_state] += reward
            for j in visited_states:
                visits[j] += 1

        for state in visits.keys():
            if visits[state] != 0:
                values[state] /= visits[state]

        return values


class EveryVisitMonteCarlo:
    def __init__(self, env):
        self.env = env

    def simulate_with_trajectories(self, trajectories):
        net_values = {state: 0 for state in self.env.P.keys()}
        net_visits = {state: 0 for state in self.env.P.keys()}

        for trajectory in trajectories:
            values = {state: 0 for state in self.env.P.keys()}
            visits = {state: 0 for state in self.env.P.keys()}
            states_visited = []
            rewards = []

            for i in range(len(trajectory) // 2):
                state = trajectory[i * 2]
                reward = trajectory[i * 2 + 1]
                next_state = trajectory[i * 2 + 2]

                states_visited.append(state)
                rewards.append(reward)

                state = next_state

            total_return = 0
            for t in range(len(states_visited) - 1, -1, -1):
                state = states_visited[t]
                total_return += rewards[t]
                visits[state] += 1
                values[state] += total_return

            for state in self.env.P.keys():
                net_values[state] += values[state]
                net_visits[state] += visits[state]
            print(net_values)
            print(net_visits)

        for state in self.env.P.keys():
            if net_visits[state] != 0:
                net_values[state] /= net_visits[state]

        return net_values


# Create an instance of the environment
env = Environment(P)

# Example trajectories
trajectories = [
    ['A', 1, 'B', 1, 'C', 1, 'D', 1, 'E', 1, 'F', 10, 'G'],
    ['A', 1, 'B', 1, 'C', 4, 'E', 1, 'F', 10, 'G'],
    ['A', 1, 'B', 1, 'C', 4, 'E', -2, 'D', 1, 'E', 1, 'F', 10, 'G'],
    ['A', 1, 'B', 1, 'C', 4, 'E', -2, 'D', 1, 'E', 1, 'F', -2, 'E', 1, 'F', 10, 'G'],
    ['A', 1, 'B', 1, 'C', -2, 'B', 1, 'C', 1, 'D', 1, 'E', 1, 'F', 10, 'G']
]


# Run simulation with trajectories for First-Visit Monte Carlo
first_visit_monte_carlo_agent = FirstVisitMonteCarlo(env)
first_visit_values = first_visit_monte_carlo_agent.simulate_with_trajectories(trajectories)
print("First-Visit Monte Carlo Values:", first_visit_values)





# Run simulation with trajectories for Every-Visit Monte Carlo
every_visit_monte_carlo_agent = EveryVisitMonteCarlo(env)
every_visit_values = every_visit_monte_carlo_agent.simulate_with_trajectories(trajectories)
print("Every-Visit Monte Carlo Values:", every_visit_values)






