import numpy
import matplotlib.pyplot as plt
import random
from IPython.display import clear_output





class TicTacToe:
    def __init__(self, starting_player=None):
        self.board = [[' ' for _ in range(3)] for _ in range(3)]
        self.current_player = starting_player if starting_player in ['X', 'O'] else random.choice(['X', 'O'])

    def act(self, move):
        row, col = move
        if self.is_valid_move(row, col):
            self.board[row][col] = self.current_player
            self.toggle_player()
        else:
            print("Invalid move. Try again.")
            
    def is_valid_move(self, row, col):
        return 0 <= row < 3 and 0 <= col < 3 and self.board[row][col] == ' '

    def toggle_player(self):
        self.current_player = 'O' if self.current_player == 'X' else 'X'
        
    def print_board(self):
        clear_output(wait=True) #clears output for clean interface
        for row in self.board:
            print('|'.join(row))
            print('-' * 5)

    def is_winner(self, symbol):
        for i in range(3):
            if all(self.board[i][j] == symbol for j in range(3)) or all(self.board[j][i] == symbol for j in range(3)):
                return True
        if all(self.board[i][i] == symbol for i in range(3)) or all(self.board[i][2 - i] == symbol for i in range(3)):
            return True
        return False

    def is_game_over(self):
        return self.is_winner('X') or self.is_winner('O') or self.is_board_full()
        
    def is_board_full(self):
        return all(cell != ' ' for row in self.board for cell in row)

    def play(self):
        while not self.is_game_over():
            self.print_board()
            print(f"Player {self.current_player}'s turn.")
            row, col = map(int, input("Enter the row and column (separated by comma): ").split(','))
            self.act((row, col))
        self.print_board()
        if self.is_winner('X'):
            print("Player X wins!")
        elif self.is_winner('O'):
            print("Player O wins!")
        else:
            print("It's a draw!")
            


starting_player = input("Choose starting player (X or O, or leave blank for random): ")
game = TicTacToe(starting_player.upper())
game.play()








class TicTacToe:
    def __init__(self, starting_player='X'):
        self.board = [[' ' for _ in range(3)] for _ in range(3)]
        self.current_player = starting_player

    def act(self, move):
        row, col = move
        if self.is_valid_move(row, col):
            self.board[row][col] = self.current_player
            self.toggle_player()
        else:
            print("Invalid move. Try again.")
            
    def is_valid_move(self, row, col):
        return 0 <= row < 3 and 0 <= col < 3 and self.board[row][col] == ' '

    def toggle_player(self):
        self.current_player = 'O' if self.current_player == 'X' else 'X'
        
    def print_board(self):
        for row in self.board:
            print('|'.join(row))
            print('-' * 5)
#######################################################################################
    def is_winner(self, symbol):
        for i in range(3):
            if all(self.board[i][j] == symbol for j in range(3)) or all(self.board[j][i] == symbol for j in range(3)):
                return True
        if all(self.board[i][i] == symbol for i in range(3)) or all(self.board[i][2 - i] == symbol for i in range(3)):
            return True
        return False

    def is_game_over(self):
        return self.is_winner('X') or self.is_winner('O') or self.is_board_full()
        
    def is_board_full(self):
        return all(cell != ' ' for row in self.board for cell in row)

#######################################################################################################
    def random_agent_pick(self):
        empty_squares = [(i, j) for i in range(3) for j in range(3) if self.board[i][j] == ' ']
        return random.choice(empty_squares) if empty_squares else None

    def safe_agent_pick(self):
        for i in range(3):   # Check if picking this square leads to a win 
            for j in range(3):
                if self.is_valid_move(i, j):
                    self.board[i][j] = 'O'
                    if self.is_winner('O'):
                        self.board[i][j] = ' '
                        return i, j
                    self.board[i][j] = ' '

        for i in range(3):   # Check if picking this square makes others win
            for j in range(3):
                if self.is_valid_move(i, j):
                    self.board[i][j] = 'X'
                    if self.is_winner('X'):
                        self.board[i][j] = ' '
                        return i, j
                    self.board[i][j] = ' '

        return self.random_agent_pick() # If no winning or blocking moves, behave like the random agent
#############################################################################################################
    def play(self, opponent='O', agent='random'):
        while not self.is_game_over():
            self.print_board()
            print(f"Player {self.current_player}'s turn.")
            
            if self.current_player == opponent:
                row, col = map(int, input("Enter the row and column (separated by comma): ").split(','))
                self.act((row, col))
                
            else:  # It's the agent's turn
                if agent == 'random':
                    move = self.random_agent_pick()
                elif agent == 'safe':
                    move = self.safe_agent_pick()
                if not move:
                    return "no possible moves"
                self.act(move)

        self.print_board()
        if self.is_winner('X'):
            print("Player X wins!")
        elif self.is_winner('O'):
            print("Player O wins!")
        else:
            print("It's a draw!")





class TicTacToeQLearning:
    def __init__(self, epsilon=0.2, alpha=0.4, gamma=0.9):
        self.q_table = {}
        self.epsilon = epsilon
        self.alpha = alpha
        self.gamma = gamma

    def state_to_string(self, state):
        return ''.join([str(cell) for row in state for cell in row])

    def is_valid_action(self, state, action):
        row, col = divmod(action, 3)
        return state[row][col] == ' '

    def is_winner(self,state,player):
        for i in range(3):
            if all(state[i][j] == player for j in range(3)) or all(state[j][i] == player for j in range(3)):
                return True
        if all(state[i][i] == player for i in range(3)) or all(state[i][2 - i] == player for i in range(3)):
            return True
        return False

    def is_full(self,state):
        for i in range(3):
            for j in range(3):
                if state[i][j] != ' ':
                    return False
        return True
        
    def get_q_value(self, state, action): #pass only valid things 
        state_str = self.state_to_string(state)

        #initialization
        if state_str not in self.q_table:
            self.q_table[state_str] = {}
            for i in range(9):
                if self.is_valid_action(state, i):
                    self.q_table[state_str][i] = 0.0
                    
        return self.q_table[state_str][action]

    
    def choose_action(self, state, valid_actions):
        if random.uniform(0, 1) < self.epsilon:
            return random.choice(valid_actions)
        else:
            state_str = self.state_to_string(state)
            ls = [ (i[0]*3 + i[1] , self.get_q_value(state, i[0]*3 + i[1])) for i in valid_actions ]
            max_q_value = max(ls, key=lambda x: x[1])[1]
            best_actions = [action for action, q_value in ls if q_value == max_q_value]
            chosen_action = random.choice(best_actions)
            return [chosen_action // 3,chosen_action % 3]

    def update_q_value(self, state, action, next_state, reward):
        state_str = self.state_to_string(state)
        current_q_value = self.get_q_value(state, action)
        next_state_str = self.state_to_string(next_state)
        
        max_next_q_value = 0.0
        if next_state_str  in self.q_table:
            max_next_q_value = max(self.q_table[next_state_str].values(),default = 0.0) #if no possibility meaning terminal so 0
        new_q_value = current_q_value + self.alpha * (reward + self.gamma * max_next_q_value - current_q_value)
        self.q_table[state_str][action] = new_q_value



def play_game(q_agent,env,adversary,training=True):
    i = 0
    prev_state = [row.copy() for row in env.board]
    _action = None
    reward = 0
    current_state = [row.copy() for row in env.board]  
    start_state = env.current_player
    while not env.is_game_over():
        valid_actions = [[i, j] for i in range(3) for j in range(3) if env.board[i][j] == ' ']
        if env.current_player == 'X':    
            state = [row.copy() for row in env.board]# Q-learning agent's turn
            action = q_agent.choose_action(state, valid_actions)
            _action = action
        else:                                          
            if adversary == 'random':
                action = env.random_agent_pick()
            elif adversary == 'safe':
                action = env.safe_agent_pick()
            else:
                choice = random.choice(['random','safe'])
                if choice == 'random':
                    action = env.random_agent_pick()
                elif choice == 'safe':
                    action = env.safe_agent_pick()
                    
        env.act(action)
        if env.is_winner('X'):
            reward += 1
        elif env.is_winner('O'):
            reward += -1
        elif env.is_board_full():
            reward += 0.3 # if draw
            
        if training and env.current_player == 'X' and (start_state == 'X' or i > 0): # this is just for handling edge case for random initialization
            prev_state = current_state
            current_state = [row.copy() for row in env.board]
            action_index = _action[0]*3  +  _action[1]
            q_agent.update_q_value(prev_state,action_index,current_state,reward)
            reward = 0 # reset reward to store again after updating
        i += 1
        # -----------------
    # this last is for an edge case where we miss the update then finishes the game
    if training and env.current_player == 'O':  # here we are upating for every second round as we need only after state
        prev_state = current_state
        current_state = [row.copy() for row in env.board]
        action_index = _action[0]*3  +  _action[1]
        q_agent.update_q_value(prev_state,action_index,current_state,reward)
        reward = 0 # reset the reawrd for next turn
    
    status = 'draw'
    if env.is_winner('X'):
        status = 'win'
    elif env.is_winner('O'):
        status = 'loss'
    return status


def evaluate_agent(q_agent,noof_games = 100):
    random_wins = 0
    random_loss = 0
    random_draws = 0
    safe_wins = 0
    safe_loss = 0
    safe_draws = 0
    for _ in range(noof_games):
        # -----------------------
        env = TicTacToe(starting_player='X')
        status = play_game(q_agent,env,'random', training=False)
        if status == 'win':
            random_wins += 1
        elif status == 'loss':
            random_loss += 1
        else :
            random_draws += 1
        # ----------------------  
        env = TicTacToe(starting_player='X')
        status = play_game(q_agent,env,'safe', training=False)
        if status == 'win':
            safe_wins += 1
        elif status == 'loss':
            safe_loss += 1
        else:
            safe_draws += 1
        # ----------------------

    return random_wins,random_loss,random_draws,safe_wins,safe_loss,safe_draws


def plot_win_rates(episode_list, random_win_list, safe_win_list):
    plt.figure(figsize=(10, 6))
    for i in range(len(random_win_list)):
        random_win_list[i] /= 100
        safe_win_list[i] /= 100
    plt.plot(episode_list, random_win_list, label='Win rate against Random agent', marker='o')
    plt.plot(episode_list, safe_win_list, label='Win rate against Safe agent', marker='o')
    plt.ylim(0, 1)
    plt.title('Win Rates')
    plt.xlabel('Episode')
    plt.ylabel('Win Rate')
    plt.legend()
    plt.grid(True)
    plt.show()





q_agent = TicTacToeQLearning(epsilon=0.1, alpha=0.3, gamma=0.9)

episode_list = []
random_wins_list = []
safe_wins_list = []

for episode in range(1, 10001):
    env = TicTacToe(starting_player=random.choice(['X', 'O'])) # at each game starting player is randomly chosen
    play_game(q_agent, env, 'random')
    if episode % 200 == 0:
        random_wins, random_loss, random_draws, safe_wins, safe_loss, safe_draws = evaluate_agent(q_agent)
        episode_list.append(episode)
        random_wins_list.append(random_wins)
        safe_wins_list.append(safe_wins)

random_wins, random_loss, random_draws, safe_wins, safe_loss, safe_draws = evaluate_agent(q_agent,1000)

print("Wins against Random Agent:", random_wins)
print("Losses against Random Agent:", random_loss)
print("Draws against Random Agent:", random_draws)
print("Wins against Safe Agent:", safe_wins)
print("losses against Safe Agent:", safe_loss)
print("Draws against Safe Agent:", safe_draws)


plot_win_rates(episode_list,random_wins_list,safe_wins_list)





q_agent = TicTacToeQLearning(epsilon=0.1, alpha=0.35, gamma=0.99)

episode_list = []
random_wins_list = []
safe_wins_list = []

for episode in range(1, 10001):
    env = TicTacToe(starting_player=random.choice(['X', 'O'])) # at each game starting player is randomly chosen
    play_game(q_agent, env, 'safe')
    if episode % 200 == 0:
        random_wins, random_loss, random_draws, safe_wins, safe_loss, safe_draws = evaluate_agent(q_agent)
        episode_list.append(episode)
        random_wins_list.append(random_wins)
        safe_wins_list.append(safe_wins)

random_wins, random_loss, random_draws, safe_wins, safe_loss, safe_draws = evaluate_agent(q_agent,1000)

print("Wins against Random Agent:", random_wins)
print("Losses against Random Agent:", random_loss)
print("Draws against Random Agent:", random_draws)
print("Wins against Safe Agent:", safe_wins)
print("losses against Safe Agent:", safe_loss)
print("Draws against Safe Agent:", safe_draws)

plot_win_rates(episode_list,random_wins_list,safe_wins_list)





q_agent = TicTacToeQLearning(epsilon=0.15, alpha=0.3, gamma=0.99)

episode_list = []
random_wins_list = []
safe_wins_list = []

for episode in range(1, 10001):
    env = TicTacToe(starting_player=random.choice(['X', 'O'])) # at each game starting player is randomly chosen
    play_game(q_agent, env, 'both')
    if episode % 200 == 0:
        random_wins, random_loss, random_draws, safe_wins, safe_loss, safe_draws = evaluate_agent(q_agent)
        episode_list.append(episode)
        random_wins_list.append(random_wins)
        safe_wins_list.append(safe_wins)

random_wins, random_loss, random_draws, safe_wins, safe_loss, safe_draws = evaluate_agent(q_agent,1000)
print("Wins against Random Agent:", random_wins)
print("Losses against Random Agent:", random_loss)
print("Draws against Random Agent:", random_draws)
print("Wins against Safe Agent:", safe_wins)
print("losses against Safe Agent:", safe_loss)
print("Draws against Safe Agent:", safe_draws)

plot_win_rates(episode_list,random_wins_list,safe_wins_list)












