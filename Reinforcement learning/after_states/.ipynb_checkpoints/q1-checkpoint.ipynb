{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dee491b-bf4e-44df-88bb-3641f6ff26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af8849-e8fd-4ee5-bf6a-40313f2a1782",
   "metadata": {},
   "source": [
    "## Question 1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c91338-1a61-444d-bd5a-fa2f7fa50dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, P, gamma=1.0):\n",
    "        self.P = P\n",
    "        self.gamma = gamma\n",
    "        self.terminal_states = self.find_terminal_states()\n",
    "\n",
    "    def find_terminal_states(self):\n",
    "        terminal_states = set()\n",
    "\n",
    "        for state, outcomes in self.P.items():\n",
    "            is_terminal = all(isinstance(outcome, tuple) and outcome[2] == state for action, outcomes in outcomes.items() for outcome in outcomes[action])\n",
    "            if is_terminal:\n",
    "                terminal_states.add(state)\n",
    "\n",
    "        return list(terminal_states)\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        return list(self.P[state].keys())\n",
    "\n",
    "    def get_next_state_info(self, state, action):\n",
    "        return self.P[state][action]\n",
    "\n",
    "    def random_state(self):\n",
    "        return np.random.choice(list(self.P.keys()))\n",
    "\n",
    "# Define the transition probabilities (P) for the example\n",
    "P = {\n",
    "    'A': {0: [(1.0, 0, 'A', False)], 1: [(1.0, 1, 'B', False)]},\n",
    "    'B': {0: [(1.0, -2, 'A', False)], 1: [(1.0, 1, 'C', False)]},\n",
    "    'C': {0: [(1.0, -2, 'B', False)], 1: [(0.5, 1, 'D', False), (0.5, 4, 'E', False)]},\n",
    "    'D': {0: [(1.0, -2, 'C', False)], 1: [(1.0, 1, 'E', False)]},\n",
    "    'E': {0: [(1.0, -2, 'D', False)], 1: [(1.0, 1, 'F', False)]},\n",
    "    'F': {0: [(1.0, -2, 'E', False)], 1: [(1.0, 1, 'G', True)]},\n",
    "    'G': {0: [(1.0, 0, 'G', True), (1.0, 0, 'G', True)]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1975fbe-565c-490f-96ec-c4c0b9c9e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstVisitMonteCarlo:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def simulate_with_trajectories(self, trajectories):\n",
    "        values = {state: 0 for state in self.env.P.keys()}\n",
    "        visits = {state: 0 for state in self.env.P.keys()}\n",
    "\n",
    "        for trajectory in trajectories:\n",
    "            visited_states = set()\n",
    "            for i in range(len(trajectory)//2 ): \n",
    "                state = trajectory[i*2]\n",
    "                reward = trajectory[i*2 + 1]\n",
    "                if state not in visited_states:\n",
    "                    visited_states.add(state)\n",
    "                for visited_state in visited_states:\n",
    "                    values[visited_state] += reward\n",
    "            for j in visited_states:\n",
    "                visits[j] += 1\n",
    "\n",
    "        for state in visits.keys():\n",
    "            if visits[state] != 0:\n",
    "                values[state] /= visits[state]\n",
    "\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cfa1eb3-9bd1-41bf-a341-974da6dd9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EveryVisitMonteCarlo:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def simulate_with_trajectories(self, trajectories):\n",
    "        net_values = {state: 0 for state in self.env.P.keys()}\n",
    "        net_visits = {state: 0 for state in self.env.P.keys()}\n",
    "\n",
    "        for trajectory in trajectories:\n",
    "            values = {state: 0 for state in self.env.P.keys()}\n",
    "            visits = {state: 0 for state in self.env.P.keys()}\n",
    "            states_visited = []\n",
    "            rewards = []\n",
    "\n",
    "            for i in range(len(trajectory) // 2):\n",
    "                state = trajectory[i * 2]\n",
    "                reward = trajectory[i * 2 + 1]\n",
    "                next_state = trajectory[i * 2 + 2]\n",
    "\n",
    "                states_visited.append(state)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            total_return = 0\n",
    "            for t in range(len(states_visited) - 1, -1, -1):\n",
    "                state = states_visited[t]\n",
    "                total_return += rewards[t]\n",
    "                visits[state] += 1\n",
    "                values[state] += total_return\n",
    "\n",
    "            for state in self.env.P.keys():\n",
    "                net_values[state] += values[state]\n",
    "                net_visits[state] += visits[state]\n",
    "            print(net_values)\n",
    "            print(net_visits)\n",
    "\n",
    "        for state in self.env.P.keys():\n",
    "            if net_visits[state] != 0:\n",
    "                net_values[state] /= net_visits[state]\n",
    "\n",
    "        return net_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d37c8e83-ad9a-4323-891f-2995a0219e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the environment\n",
    "env = Environment(P)\n",
    "\n",
    "# Example trajectories\n",
    "trajectories = [\n",
    "    ['A', 1, 'B', 1, 'C', 1, 'D', 1, 'E', 1, 'F', 10, 'G'],\n",
    "    ['A', 1, 'B', 1, 'C', 4, 'E', 1, 'F', 10, 'G'],\n",
    "    ['A', 1, 'B', 1, 'C', 4, 'E', -2, 'D', 1, 'E', 1, 'F', 10, 'G'],\n",
    "    ['A', 1, 'B', 1, 'C', 4, 'E', -2, 'D', 1, 'E', 1, 'F', -2, 'E', 1, 'F', 10, 'G'],\n",
    "    ['A', 1, 'B', 1, 'C', -2, 'B', 1, 'C', 1, 'D', 1, 'E', 1, 'F', 10, 'G']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2104fa16-bc7c-436b-aea5-6c921bfabead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-Visit Monte Carlo Values: {'A': 15.4, 'B': 14.4, 'C': 13.4, 'D': 11.75, 'E': 10.4, 'F': 9.8, 'G': 0}\n"
     ]
    }
   ],
   "source": [
    "# Run simulation with trajectories for First-Visit Monte Carlo\n",
    "first_visit_monte_carlo_agent = FirstVisitMonteCarlo(env)\n",
    "first_visit_values = first_visit_monte_carlo_agent.simulate_with_trajectories(trajectories)\n",
    "print(\"First-Visit Monte Carlo Values:\", first_visit_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45b93f-e45c-43c7-ae65-1df2b3a988d2",
   "metadata": {},
   "source": [
    "## Question 1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33303a31-0bc1-4033-8bbd-059a19b9bdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 15, 'B': 14, 'C': 13, 'D': 12, 'E': 11, 'F': 10, 'G': 0}\n",
      "{'A': 1, 'B': 1, 'C': 1, 'D': 1, 'E': 1, 'F': 1, 'G': 0}\n",
      "{'A': 32, 'B': 30, 'C': 28, 'D': 12, 'E': 22, 'F': 20, 'G': 0}\n",
      "{'A': 2, 'B': 2, 'C': 2, 'D': 1, 'E': 2, 'F': 2, 'G': 0}\n",
      "{'A': 48, 'B': 45, 'C': 42, 'D': 24, 'E': 43, 'F': 30, 'G': 0}\n",
      "{'A': 3, 'B': 3, 'C': 3, 'D': 2, 'E': 4, 'F': 3, 'G': 0}\n",
      "{'A': 63, 'B': 59, 'C': 55, 'D': 35, 'E': 73, 'F': 49, 'G': 0}\n",
      "{'A': 4, 'B': 4, 'C': 4, 'D': 3, 'E': 7, 'F': 5, 'G': 0}\n",
      "{'A': 77, 'B': 86, 'C': 80, 'D': 47, 'E': 84, 'F': 59, 'G': 0}\n",
      "{'A': 5, 'B': 6, 'C': 6, 'D': 4, 'E': 8, 'F': 6, 'G': 0}\n",
      "Every-Visit Monte Carlo Values: {'A': 15.4, 'B': 14.333333333333334, 'C': 13.333333333333334, 'D': 11.75, 'E': 10.5, 'F': 9.833333333333334, 'G': 0}\n"
     ]
    }
   ],
   "source": [
    "# Run simulation with trajectories for Every-Visit Monte Carlo\n",
    "every_visit_monte_carlo_agent = EveryVisitMonteCarlo(env)\n",
    "every_visit_values = every_visit_monte_carlo_agent.simulate_with_trajectories(trajectories)\n",
    "print(\"Every-Visit Monte Carlo Values:\", every_visit_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1519ef-a39c-4371-98e4-9e48550a93e4",
   "metadata": {},
   "source": [
    "<center><img src = \"trajectories.png\" width = 600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46777f17-41ec-4509-9366-2357b420a9a5",
   "metadata": {},
   "source": [
    "<p>When ever in the trajectories we have the same state occured twice we can expert first and every visit monte carlo yields different reslt.But if we sample sufficiently enough both can give same results.In the above image it is clear that B repeated twice so we get sllightly different value for B.Similarly C(repaeted 1st),E(repeated in 4th and 5th trajectories) also changed slightly,F (repeated in 5th but very close in trajectories so very less change),A,D,G are same</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
