


import gym
import numpy as np
import matplotlib.pyplot as plt
import random
import cv2
import seaborn as sns


!pip install gym


env = gym.make('FrozenLake-v1',render_mode="rgb_array_list",is_slippery=True)
env.reset()


num_states = env.observation_space.n
num_actions = env.action_space.n
print(env.observation_space)
print(env.action_space)
env.P


# modifing rewards

for s in range(num_states):
    for a in range(num_actions):
        for k in range(len(env.P[i][j])):
            prob,state,reward,terminal = env.P[i][j][k]
            if( terminal == True):
                terminal_state[state] = 1
            if( terminal == True and abs(reward-0.0) < 1e-3 ):
                if(state == 15):
                    env.P[i][j][k] = (prob,state,5.0,terminal)
                else:
                    env.P[i][j][k] = (prob,state,-1.0,terminal)
                    
            if( terminal == False and abs(reward-0.0) < 1e-3 and state != 0 ):#fronzen part 
                env.P[i][j][k] = (prob,state,-0.01,terminal)

env.P



class valueiteration():        
    def __init__(self,env,max_epochs = 1000,epsilon = 1e-10):
        self.env = env
        self.max_epochs = max_epochs
        self.epsilon = epsilon
        self.state_value = np.zeros(env.observation_space.n)
        self.num_states = env.observation_space.n
        self.num_actions = env.action_space.n
        self.policy = np.zeros(self.num_states)
        
    def retrieve_policy(self):
        #given optimal value_function we can find optimal policy we can many
        optimal_action = np.zeros(self.num_states)*-1
        for i in range(self.num_states):
            actions = np.zeros(self.num_actions)
            for j in range(self.num_actions):
                gain = 0
                for k in self.env.P[i][j]:
                    gain += k[0]*self.state_value[k[1]]
                actions[j] = gain
            optimal_action[i] = np.argmax(actions)
       
        # Plotting the heatmap
        sns.heatmap(self.state_value.reshape(4,4), cmap='Blues', annot=True, fmt='.2f')
        self.policy = optimal_action
        action = optimal_action.reshape(4,4)
        print(action)
        action = action.T
        for i in range(4):
            for j in range(4):
                if(terminal_state[j*4 + i]): # as the matrix is transposed due to plt.arrow characteristics
                    continue
                move = action[i][j]
                dx,dy = 0,0
                if move == 0:  # Move left
                    dx = 0.3
                elif move == 1:  # Move down
                    dy = 0.3
                elif move == 2:  # Move right
                    dx = -0.3
                elif move == 3:  # Move up
                    dy = -0.3
                #  arrow top left is origin
                plt.arrow(i + 0.5,j + 0.5, dx, dy, width=0.02, head_width=0.15, head_length=0.15, fc='black', ec='black')

        plt.show()
           
        
    def iteration(self,gamma):
        initial = self.state_value.copy()
        for i in range(self.num_states):
            temp = self.state_value[i]
            actions = np.zeros(self.num_actions)
            for j in range(self.num_actions):
                summation = 0.0
                for k in self.env.P[i][j]: # each k is a tuple
                    summation += k[0]*self.state_value[k[1]]
                actions[j] = summation
            self.state_value[i] = k[2]+gamma*np.max(actions) #update statement  
        return np.linalg.norm(initial - self.state_value)
                         
        
    def train(self,gamma):
        error = []
        delta = self.iteration(gamma)
        epochs = 1
        error.append(delta)
        while delta > self.epsilon:
            delta = self.iteration(gamma)
            epochs += 1
            error.append(delta)
            
        #plotting convergence
        print(f'Noof epochs {epochs}')
        plt.figure(figsize=(4, 4))
        x_vals = range(epochs)
        plt.plot(x_vals, error)
        plt.xlabel('Epochs')
        plt.ylabel('change in state value')
        plt.grid()
        plt.title(f'Error Convergence with gamma as {gamma}')
        plt.show() 





class policy_iteration:
    def __init__(self,env,max_epochs = 1000,epsilon = 1e-9):
        self.env = env
        self.max_epochs = max_epochs
        self.epsilon = epsilon
        self.num_states = env.observation_space.n
        self.num_actions = env.action_space.n
        
    def policy_eval(self,policy,gamma):
        val_fun = np.zeros(self.num_states)
        for noof_epoch in range(self.max_epochs):
            norm_diff = 0
            for state in range(self.num_states):
                action = policy[state]
                initial = val_fun[state]
                temp = 0
                for prob,next_state,reward,terminal in self.env.P[state][action]:
                    temp += prob*(reward + gamma*val_fun[next_state])
                val_fun[state] = temp
                norm_diff = abs(temp - initial)
                
            if norm_diff < self.epsilon:
                break
                
        return val_fun
    
    def policy_improvement(self,policy,value_fun):
        new_policy = np.zeros(self.num_states)
        for state in range(self.num_states):
            Q_vals = np.zeros(self.num_actions) #vector of q values
            for action in range(self.num_actions):
                for prob, next_state, reward, done in self.env.P[state][action]:
                    Q_vals[action] += prob*(reward + gamma * value_fun[next_state])
            
            # Update the policy 
            optimal_action = np.argmax(Q_vals)
            new_policy[state] = optimal_action # greedy action
        return new_policy
        
        
    def train(self,gamma):
        policy = np.random.randint(0,3)
        converged = False
        epochs = 0
        while !converged or max_epochs < epochs:
            value_fun = policy_eval(policy,gamma)
            new_policy = policy_improvement(policy,value_fun,gamma)
            for i in range(num_states):
                if(new_policy[i] != policy[i]):
                    policy = new_policy
                    epochs += 1
                    continue
            converged = True
            epochs += 1
        return policy
        


model = policyiteration(env)
policy = model.train(0.9)



from gym import spaces

class CustomEnv(gym.Env):
    def __init__(self):
        super(CustomGridEnv, self).__init__()

        self.grid_size = 5
        self.observation_space = spaces.Discrete(self.grid_size * self.grid_size)
        self.action_space = spaces.Discrete(4) 
        self.start_state = (3, 0) 
        self.terminal_states = [(2, 2), (2, 4)]
        self.rewards[(2,2)] = 1
        self.rewards[(2,4)] = 10
        for col in range(self.grid_size):
            self.terminal_states.append((self.grid_size - 1, col))
            self.rewards[(self.grid_size - 1, col)] = -10
            
        # Define transition probabilities
        self.transition_prob = 0.8
        # Define action to direction mapping
        self._action_to_direction = {
            0: np.array([0, -1]),  # Left
            1: np.array([0, 1]),   # Right
            2: np.array([-1, 0]),  # Up
            3: np.array([1, 0])    # Down
        }
        # Initialize the state
        self.state = self.start_state

    def reset(self):
        self.state = self.start_state
        return self.state

    def step(self, action):

        direction = self._action_to_direction[action]
        intended_next_state = tuple(np.clip(np.array(self.state) + direction, [0, 0], [self.grid_size - 1, self.grid_size - 1]))
        # Create a set to store valid next states possible except intended state
        valid_next_states = set()
        for dxn in self._action_to_direction.values():
            if dxn == direction:
                continue
            next_state = tuple(np.clip(np.array(self.state) + dxn, [0, 0] , [self.grid_size - 1, self.grid_size - 1]))
            valid_next_states.add(next_state)

        # Calculate the next state probabilistically
        if random.uniform(0, 1) > self.transition_prob:
            next_state = random.choice(list(valid_next_states))
        else:
            next_state = intended_next_state
            
        if next_state in self.terminal_states:
            reward = self.rewards[next_state]
            done = True
        else:
            reward = 0
            done = False
            
        self.state = next_state #updating state
        return next_state,reward,done,{}

    def render(self):
        grid = []
        for _ in range(self.grid_size):
            row = ['_' for _ in range(self.grid_size)]
            grid.append(row)
    
        for terminal_state, reward in self.rewards.items():
            grid[terminal_state[0]][terminal_state[1]] = str(reward)
    
        grid[self.state[0]][self.state[1]] = 'X'
    
        for row in grid:
            print(' '.join(row))
        print()
        return


# Example usage
env = CustomGridEnv()
env.reset()
for _ in range(10):
    action = env.action_space.sample()  # Random action
    next_state, reward, done, _ = env.step(action)
    env.render()
    if done:
        break
