{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3d2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bcde026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#consider a simple function \n",
    "\n",
    "def fun(x): #x is tensor\n",
    "    return 10*(x[0]**2) + (x[1]**2)\n",
    "\n",
    "def grad_fun(x): #x is tensor\n",
    "    return torch.tensor([20*x[0],2*x[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785c919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(tracker, filename):\n",
    "    x_points, y_points,discription = tracker\n",
    "    images = []\n",
    "    for i in range(len(x_points)):\n",
    "\n",
    "        plt.scatter(x_points[0:i+1], y_points[0:i+1],s=14, c=\"red\")\n",
    "        # Generate a meshgrid for plotting\n",
    "        xvals = np.linspace(-2, 2, 100)\n",
    "        yvals = np.linspace(-2, 2, 100)\n",
    "        X, Y = np.meshgrid(xvals, yvals)\n",
    "        Z = np.zeros_like(X)\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                Z[i, j] = fun(torch.tensor([X[i, j], Y[i, j]]))\n",
    "\n",
    "        # Plot the contour lines and tracing\n",
    "        plt.contour(X, Y, Z, levels=12)\n",
    "\n",
    "        # Label the axes and title\n",
    "        plt.xlabel(\"x[0]\")\n",
    "        plt.ylabel(\"x[1]\")\n",
    "        plt.text(0, 1.8,discription, fontsize = 9)\n",
    "        plt.savefig(f\"temp_{i}.png\",transparent = False, facecolor = 'white')\n",
    "        im =imageio.v2.imread(f'temp_{i}.png')\n",
    "        images.append(im)\n",
    "        plt.close()\n",
    "    imageio.mimsave(filename, images, fps=1 ,loop = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce46d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_SGD(alpha = 0.01,max_epochs=100):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    x = 2*torch.rand(2)  #initialization\n",
    "    x_points.append(x[0])\n",
    "    y_points.append(x[1])\n",
    "    for epoch in range(max_epochs):\n",
    "#         if(torch.norm(grad_fun(x)) < 1e-6):\n",
    "#             break;\n",
    "        x = x - alpha*grad_fun(x) \n",
    "        x_points.append(x[0])\n",
    "        y_points.append(x[1])\n",
    "    discription = f\"vanilla_SGD,lr is {alpha}\"\n",
    "    tracker = [x_points,y_points,discription]\n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9752d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_moment(alpha = 0.01,gamma=0.9,max_epochs=100):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    x = 2*torch.rand(2)\n",
    "    update =torch.zeros(2) #initialization\n",
    "    x_points.append(x[0])\n",
    "    y_points.append(x[1])\n",
    "    for epoch in range(max_epochs):\n",
    "#         if(torch.norm(grad_fun(x)) < 1e-6):\n",
    "#             break;\n",
    "        update = gamma*update + alpha*grad_fun(x)\n",
    "        x = x - update\n",
    "        x_points.append(x[0])\n",
    "        y_points.append(x[1])\n",
    "    discription = f\"SGD_moment,lr is {alpha},gamma is {gamma}\"\n",
    "    tracker = [x_points,y_points,discription]\n",
    "    return tracker\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786fd37",
   "metadata": {},
   "source": [
    "# SGD with moment\n",
    "\n",
    "<p>SGD with momentum (SGD+Momentum) has several benefits over plain SGD (Stochastic Gradient Descent). Here are some of the advantages of using SGD+Momentum:</p>\n",
    "\n",
    "* Faster Convergence: SGD+Momentum converges faster by leveraging accumulated gradients to accelerate the optimization process and escape shallow local minima.\n",
    "* Improved Robustness to Noise: The momentum term in SGD+Momentum smooths out noisy gradients, making the optimization process more robust and reliable.\n",
    "* Enhanced Exploration of Solution Space: SGD+Momentum explores complex solution spaces more efficiently, enabling the optimizer to escape local minima and reach better optima.\n",
    "* Dampening Oscillations: The momentum term dampens oscillations, preventing overshooting and oscillating around the minimum, resulting in a more stable optimization process.\n",
    "* Reduced Learning Rate Sensitivity: SGD+Momentum is less sensitive to the choice of learning rate, allowing for larger learning rates without causing instability, reducing the need for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bdd8c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_Nesterov_Momentum(alpha = 0.01,gamma=0.9,max_epochs=100):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    x = 2*torch.rand(2)\n",
    "    update =torch.zeros(2) #initialization\n",
    "    x_points.append(x[0])\n",
    "    y_points.append(x[1])\n",
    "    for epoch in range(max_epochs):\n",
    "#         if(torch.norm(grad_fun(x)) < 1e-6):\n",
    "#             break;\n",
    "        x_ahead = x - gamma*update\n",
    "        update = gamma*update + alpha*grad_fun(x_ahead)\n",
    "        x = x - update\n",
    "        x_points.append(x[0])\n",
    "        y_points.append(x[1])\n",
    "    \n",
    "    discription = f\"SGD_Nesterov_Momentum,lr is {alpha},gamma is {gamma}\"\n",
    "    tracker = [x_points,y_points,discription]\n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c90d8",
   "metadata": {},
   "source": [
    "# SGD_Nesterov_Momentum\n",
    "<p>benefits of using SGD with Nesterov momentum (Nesterov accelerated gradient or NAG) over \n",
    "SGD with momentum:</p>\n",
    "\n",
    "* Enhanced Gradient Estimation: Nesterov momentum provides a more accurate estimation of the gradient by considering the momentum term and calculating the gradient ahead of the current position.\n",
    "* Improved Convergence Speed: Nesterov momentum converges faster due to its lookahead gradient estimation, allowing for larger and more effective steps towards the minimum.\n",
    "* Better Handling of Sharp Turns: Nesterov momentum performs better in areas with sharp turns or high-curvature landscapes by anticipating and adjusting its trajectory ahead of time, preventing overshooting and oscillations.\n",
    "* Enhanced Robustness to Noise: Nesterov momentum is more robust to noisy gradients, similar to SGD with momentum, by smoothing out irregularities in the gradient updates.\n",
    "* Theoretical Convergence Guarantees: Nesterov momentum has theoretical guarantees for convex functions, ensuring a faster convergence rate compared to plain SGD and SGD with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eede3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaGrad(alpha = 0.1,epsilon = 0.01,max_epochs=100):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    x = 2*torch.rand(2)\n",
    "    v =torch.zeros(1) #initialization\n",
    "    x_points.append(x[0])\n",
    "    y_points.append(x[1])\n",
    "    for epoch in range(max_epochs):\n",
    "#         if(torch.norm(grad_fun(x)) < 1e-6):\n",
    "#             break;\n",
    "        v = v + (grad_fun(x)@grad_fun(x)) \n",
    "        x = x - (alpha/(torch.sqrt(v+epsilon)))*grad_fun(x)\n",
    "        x_points.append(x[0])\n",
    "        y_points.append(x[1])\n",
    "    discription = f\"AdaGrad,lr is {alpha},epsilon is {epsilon}\"\n",
    "    tracker = [x_points,y_points,discription]\n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e40be",
   "metadata": {},
   "source": [
    "# AdaGrad\n",
    "\n",
    "<p>benefits of using AdaGrad (Adaptive Gradient) optimization algorithm:</p>\n",
    "\n",
    "* Adaptive Learning Rates: AdaGrad adapts the learning rate for each parameter based on their historical gradients. It assigns larger learning rates to parameters with infrequent updates and smaller learning rates to parameters with frequent updates. This adaptive learning rate scheme allows AdaGrad to effectively navigate different parts of the optimization landscape, leading to efficient convergence.\n",
    "* Automatic Scaling: AdaGrad automatically scales the learning rate based on the magnitude of gradients. It divides the learning rate by the square root of the sum of squared gradients, which effectively normalizes the updates for each parameter. This feature is beneficial when dealing with data with varying scales or when optimizing models with parameters of significantly different magnitudes.\n",
    "* Robustness to Sparse Features: AdaGrad performs well in scenarios with sparse features. Since AdaGrad accumulates squared gradients over time, it naturally emphasizes features with sparse updates by allowing larger learning rates for those features. This property makes AdaGrad particularly suitable for problems such as natural language processing and recommender systems, where data often exhibits sparsity.\n",
    "* Convergence on Steep Directions: AdaGrad tends to converge faster on steep directions in the optimization landscape. By accumulating gradients over time, AdaGrad emphasizes updates along dimensions with large gradients, which can be helpful in optimizing models with steep or elongated valleys.\n",
    "* Reduced Need for Learning Rate Tuning: AdaGrad's adaptive learning rate scheme reduces the need for manual tuning of the learning rate. It automatically adjusts the learning rate based on the gradient history, mitigating the risk of using an overly large or small learning rate. This feature saves time and effort in the hyperparameter tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96bbce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSProp(alpha = 0.1,beta=0.5,epsilon = 0.01,max_epochs=100):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    x = 2*torch.rand(2)\n",
    "    v =torch.zeros(1) #initialization\n",
    "    x_points.append(x[0])\n",
    "    y_points.append(x[1])\n",
    "    for epoch in range(max_epochs):\n",
    "#         if(torch.norm(grad_fun(x)) < 1e-6):\n",
    "#             break;\n",
    "        v = beta*v + (1-beta)*(grad_fun(x)@grad_fun(x))\n",
    "        x = x - (alpha/(torch.sqrt(v+epsilon)))*grad_fun(x)\n",
    "        x_points.append(x[0])\n",
    "        y_points.append(x[1])\n",
    "    discription = f\"AdaGrad,lr is {alpha},beta is {beta},epsilon is {epsilon}\"\n",
    "    tracker = [x_points,y_points,discription]\n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acad9af",
   "metadata": {},
   "source": [
    "# RMSProp\n",
    "\n",
    "<p>RMSProp (Root Mean Square Propagation) shares some similarities with AdaGrad but incorporates a modification that addresses some of AdaGrad's limitations. Here are some benefits of using RMSProp over AdaGrad:</p>\n",
    "\n",
    "* Adaptive learning rates with momentum: Like AdaGrad, RMSProp adapts the learning rate for each parameter based on the historical gradients. However, RMSProp also introduces a momentum term that helps accelerate the convergence process. By incorporating momentum, RMSProp is able to retain information about the previous gradients and make more informed updates. This momentum term helps smooth out the optimization process, leading to faster convergence.\n",
    "\n",
    "* Better handling of velocities: AdaGrad treats all historical gradients equally, which may not be ideal in non-stationary environments where the optimal solution changes over time. RMSProp mitigates this issue by using an exponentially weighted moving average, which gives more weight to recent gradients and less weight to past gradients. This adaptability to changing environments allows RMSProp to quickly adjust the learning rate based on current gradients, facilitating more effective optimization in dynamic scenarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9895c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = vanilla_SGD(alpha = 0.01,max_epochs=100)                     # generate_gif(r2,\"r2.gif\")\n",
    "r3 = SGD_moment(alpha = 0.01,gamma=0.9,max_epochs=100)            # generate_gif(r3,\"r3.gif\")\n",
    "r4 = SGD_Nesterov_Momentum(alpha = 0.01,gamma=0.9,max_epochs=100) # generate_gif(r4,\"r4.gif\")\n",
    "r5 = AdaGrad(alpha = 1,epsilon = 0.05,max_epochs=100)             # generate_gif(r5,\"r5.gif\")\n",
    "r6 = RMSProp(alpha = 0.1,beta=0.5,epsilon = 0.01,max_epochs=100)  # generate_gif(r6,\"r6.gif\")\n",
    "\n",
    "# <img src=\"FileName.gif\" width=\"750\" align=\"center\"> to display gif in python markdown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af9e6bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparision of techiniques\n",
    "images = []\n",
    "for i in range(len(r2[0])):\n",
    "        plt.plot(r2[0][0:i+1],r2[1][0:i+1])\n",
    "        plt.plot(r3[0][0:i+1],r3[1][0:i+1])\n",
    "        plt.plot(r4[0][0:i+1],r4[1][0:i+1])\n",
    "        plt.plot(r5[0][0:i+1],r5[1][0:i+1])\n",
    "        plt.plot(r6[0][0:i+1],r6[1][0:i+1])\n",
    "        plt.legend([\"SGD\",\"SGD_moment\",\"SGD_nesterov\",\"Adagrad\",\"RMSprop\"])\n",
    "        # Generate a meshgrid for plotting\n",
    "        xvals = np.linspace(-2, 2, 100)\n",
    "        yvals = np.linspace(-2, 2, 100)\n",
    "        X, Y = np.meshgrid(xvals, yvals)\n",
    "        Z = np.zeros_like(X)\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                Z[i, j] = fun(torch.tensor([X[i, j], Y[i, j]]))\n",
    "\n",
    "        # Plot the contour lines and tracing\n",
    "        plt.contour(X, Y, Z, levels=12)\n",
    "\n",
    "        # Label the axes and title\n",
    "        plt.xlabel(\"x[0]\")\n",
    "        plt.ylabel(\"x[1]\")\n",
    "        plt.savefig(f\"temp_{i}.png\",transparent = False, facecolor = 'white')\n",
    "        im =imageio.v2.imread(f'temp_{i}.png')\n",
    "        images.append(im)\n",
    "        plt.close()\n",
    "imageio.mimsave(\"compare.gif\",images,fps=1,loop = 10)\n",
    "\n",
    "# <img src=\"./FileName.gif\" width=\"750\" align=\"center\"> to display gif in python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23d640",
   "metadata": {},
   "source": [
    "<img src=\"./compare.gif\" width=\"750\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c1e8f",
   "metadata": {},
   "source": [
    "<p>The above gif clearly illustrated the working of all methods described above</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
